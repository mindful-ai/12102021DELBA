{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr.', 'Smith', '!', 'I', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', '(', 'tomatoes', 'and', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers)\\\n",
    "from the store. Should I pick up some black-eyed peas as well?\"\n",
    "print(word_tokenize(my_text)) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Mr. Smith!', 'I’m going to buy some vegetables (tomatoes and cucumbers)from the store.', 'Should I pick up some black-eyed peas as well?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers)\\\n",
    "from the store. Should I pick up some black-eyed peas as well?\"\n",
    "print(sent_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'Mr.'), ('Mr.', 'Smith'), ('Smith', '!'), ('!', 'I'), ('I', '’'), ('’', 'm'), ('m', 'going'), ('going', 'to'), ('to', 'buy'), ('buy', 'some'), ('some', 'vegetables'), ('vegetables', '('), ('(', 'tomatoes'), ('tomatoes', 'and'), ('and', 'cucumbers'), ('cucumbers', ')'), (')', 'from'), ('from', 'the'), ('the', 'store'), ('store', '.'), ('.', 'Should'), ('Should', 'I'), ('I', 'pick'), ('pick', 'up'), ('up', 'some'), ('some', 'black-eyed'), ('black-eyed', 'peas'), ('peas', 'as'), ('as', 'well'), ('well', '?')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Mr', 'Smith', 'Should']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Mr  Smith  I’m going to buy some vegetables  tomatoes and cucumbers from the store  Should I pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "# Replace punctuations with a white space\n",
    "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers from the store  should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lower case\n",
    "clean_text = clean_text.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers from the store  should i pick up some black eyed peas as well '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove numbers\n",
    "clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>buy</th>\n",
       "      <th>cucumbers</th>\n",
       "      <th>eyed</th>\n",
       "      <th>going</th>\n",
       "      <th>hi</th>\n",
       "      <th>mr</th>\n",
       "      <th>peas</th>\n",
       "      <th>pick</th>\n",
       "      <th>smith</th>\n",
       "      <th>store</th>\n",
       "      <th>tomatoes</th>\n",
       "      <th>vegetables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
       "0      1    1          1     1      1   1   1     1     1      1      1   \n",
       "\n",
       "   tomatoes  vegetables  \n",
       "0         1           1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers)\\\n",
    "from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive: driv\n",
      "drives: driv\n",
      "driver: driv\n",
      "drivers: driv\n",
      "driven: driv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# Try some stems\n",
    "print('drive: {}'.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "from nltk.help import upenn_tagset\n",
    "upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# string decleration\n",
    "source = 'Data Science Learner'\n",
    "target = 'Data Science Learners'\n",
    "#distance calculation\n",
    "distance=nltk.edit_distance(source , target )\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TExt Format for Analysis: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.',\n",
    "'This is the second document.',\n",
    "'And the third one. One is fun.']\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "cosine([1, 1, 1, 0], [1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chai  chocolate  encoding  hot  latte  make  milk  sale  sun  today  \\\n",
       "0     0          0         0    1      0     0     0     0    1      0   \n",
       "1     0          1         0    1      0     1     1     0    0      0   \n",
       "2     0          0         1    1      0     0     0     0    0      0   \n",
       "3     1          0         0    0      1     0     1     0    0      0   \n",
       "4     0          0         0    1      0     0     0     1    0      1   \n",
       "\n",
       "   weather  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.40824829]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.40824829]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.35355339]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.33333333]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.28867513]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
    "pairs = list(combinations(range(len(corpus)),2))\n",
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  fun  is  one  second  the  third  this\n",
       "0    0         1      1    0   1    0       0    1      0     1\n",
       "1    0         1      0    0   1    0       1    1      0     1\n",
       "2    1         0      0    1   1    2       0    1      1     0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = ['This is the first document.',\n",
    "'This is the second document.',\n",
    "'And the third one. One is fun.']\n",
    "# original Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.72086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212876</td>\n",
       "      <td>0.36043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       and  document     first      fun        is      one    second  \\\n",
       "0  0.00000  0.450145  0.591887  0.00000  0.349578  0.00000  0.000000   \n",
       "1  0.00000  0.450145  0.000000  0.00000  0.349578  0.00000  0.591887   \n",
       "2  0.36043  0.000000  0.000000  0.36043  0.212876  0.72086  0.000000   \n",
       "\n",
       "        the    third      this  \n",
       "0  0.349578  0.00000  0.450145  \n",
       "1  0.349578  0.00000  0.450145  \n",
       "2  0.212876  0.36043  0.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From previous examples: checking similarity with TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chai  chocolate  encoding  hot  latte  make  milk  sale  sun  today  \\\n",
       "0     0          0         0    1      0     0     0     0    1      0   \n",
       "1     0          1         0    1      0     1     1     0    0      0   \n",
       "2     0          0         1    1      0     0     0     0    0      0   \n",
       "3     1          0         0    0      1     0     1     0    0      0   \n",
       "4     0          0         0    1      0     0     0     1    0      1   \n",
       "\n",
       "   weather  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purushotham\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chai</th>\n",
       "      <th>chocolate</th>\n",
       "      <th>encoding</th>\n",
       "      <th>hot</th>\n",
       "      <th>latte</th>\n",
       "      <th>make</th>\n",
       "      <th>milk</th>\n",
       "      <th>sale</th>\n",
       "      <th>sun</th>\n",
       "      <th>today</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580423</td>\n",
       "      <td>0.468282</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6569</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chai  chocolate  encoding       hot     latte      make      milk  \\\n",
       "0  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "1  0.000000   0.580423  0.000000  0.327000  0.000000  0.580423  0.468282   \n",
       "2  0.000000   0.000000  0.871247  0.490845  0.000000  0.000000  0.000000   \n",
       "3  0.614189   0.000000  0.000000  0.000000  0.614189  0.000000  0.495524   \n",
       "4  0.000000   0.000000  0.000000  0.370086  0.000000  0.000000  0.000000   \n",
       "\n",
       "     sale     sun   today  weather  \n",
       "0  0.0000  0.6569  0.0000   0.6569  \n",
       "1  0.0000  0.0000  0.0000   0.0000  \n",
       "2  0.0000  0.0000  0.0000   0.0000  \n",
       "3  0.0000  0.0000  0.0000   0.0000  \n",
       "4  0.6569  0.0000  0.6569   0.0000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0.23204486]]),\n",
       "  ('I make my hot chocolate with milk', 'I will have a chai latte with milk')),\n",
       " (array([[0.18165505]]),\n",
       "  ('The weather is hot under the sun', 'One hot encoding')),\n",
       " (array([[0.18165505]]), ('One hot encoding', 'There is a hot sale today')),\n",
       " (array([[0.16050661]]),\n",
       "  ('I make my hot chocolate with milk', 'One hot encoding')),\n",
       " (array([[0.1369638]]),\n",
       "  ('The weather is hot under the sun', 'There is a hot sale today')),\n",
       " (array([[0.12101835]]),\n",
       "  ('The weather is hot under the sun', 'I make my hot chocolate with milk')),\n",
       " (array([[0.12101835]]),\n",
       "  ('I make my hot chocolate with milk', 'There is a hot sale today')),\n",
       " (array([[0.]]),\n",
       "  ('The weather is hot under the sun', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]), ('One hot encoding', 'I will have a chai latte with milk')),\n",
       " (array([[0.]]),\n",
       "  ('I will have a chai latte with milk', 'There is a hot sale today'))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
    "pairs = list(combinations(range(len(corpus)),2))\n",
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# make sure the data is labeled\n",
    "import pandas as pd\n",
    "data = pd.read_table('SMSSpamCollection', header=None)\n",
    "data.columns = ['label', 'text']\n",
    "print(data.head()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  go until jurong point  crazy   available only ...\n",
      "1   ham                      ok lar    joking wif u oni   \n",
      "2  spam  free entry in   a wkly comp to win fa cup fina...\n",
      "3   ham  u dun say so early hor    u c already then say   \n",
      "4   ham  nah i don t think he goes to usf  he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# remove words with numbers, punctuation and capital letters\n",
    "import re\n",
    "import string\n",
    "alphanumeric = lambda x: re.sub(r\"\"\"\\w*\\d\\w*\"\"\", ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "data['text'] = data.text.map(alphanumeric).map(punc_lower)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into inputs and outputs\n",
    "X = data.text # inputs into model\n",
    "y = data.label # output of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go until jurong point  crazy   available only ...\n",
       "1                        ok lar    joking wif u oni   \n",
       "2    free entry in   a wkly comp to win fa cup fina...\n",
       "3    u dun say so early hor    u c already then say   \n",
       "4    nah i don t think he goes to usf  he lives aro...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "3     ham\n",
       "4     ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# test size = 30% of observations, which means training size = 70% of observations\n",
    "# random state = 42, so we all get the same random train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 6103)\n"
     ]
    }
   ],
   "source": [
    "# Numerically encode the input data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X_train_cv = cv.fit_transform(X_train) # fit_transform learns the vocab and one-hot encodes\n",
    "X_test_cv = cv.transform(X_test) # transform uses the same vocab and one-hot encodes\n",
    "# print the dimensions of the training set (text messages, terms)\n",
    "print(X_train_cv.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'spam', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "# Use a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "# Train the model\n",
    "lr.fit(X_train_cv, y_train)\n",
    "# Take the model that was trained on the X_train_cv data and apply it to the X_test_cv\n",
    "data\n",
    "y_pred_cv = lr.predict(X_test_cv)\n",
    "y_pred_cv # The output is all of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985\n",
      "Precision: 1.0\n",
      "Recall: 0.888\n",
      "F1 Score: 0.941\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_cv)\n",
    "sns.heatmap(cm, xticklabels=['predicted_ham', 'predicted_spam'], yticklabels=['actual_ham', 'actual_spam'],\n",
    "annot=True, fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\");\n",
    "\n",
    "true_neg, false_pos = cm[0]\n",
    "false_neg, true_pos = cm[1]\n",
    "accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg),3)\n",
    "precision = round((true_pos) / (true_pos + false_pos),3)\n",
    "recall = round((true_pos) / (true_pos + false_neg),3)\n",
    "f1 = round(2 * (precision * recall) / (precision + recall),3)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1 Score: {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
