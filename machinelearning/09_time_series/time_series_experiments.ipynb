{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided with a Time Series problem involving prediction of number of commuters of JetRail, a new high speed rail service by Unicorn Investors. We are provided with 2 years of data(Aug 2012-Sept 2014) and using this data we have to forecast the number of commuters for next 7 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "#Importing`a data\n",
    "df = pd.read_csv('train.csv')\n",
    "#Printing head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the print statements above, we are given 2 years of data(2012-2014) at hourly level with the number of commuters travelling and we need to estimate the number of commuters for future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will subsetting and aggregating dataset at daily basis to explain the different methods.\n",
    "\n",
    "Subsetting the dataset from (August 2012 – Dec 2013)\n",
    "Creating train and test file for modeling. The first 14 months (August 2012 – October 2013) are used as training data and next 2 months (Nov 2013 – Dec 2013) as testing data.\n",
    "Aggregating the dataset at daily basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting the dataset\n",
    "#Index 11856 marks the end of year 2013\n",
    "df = pd.read_csv('train.csv', nrows = 11856)\n",
    "\n",
    "#Creating train and test set \n",
    "#Index 10392 marks the end of October 2013 \n",
    "train=df[0:10392] \n",
    "test=df[10392:]\n",
    "\n",
    "#Aggregating the dataset at daily level\n",
    "df.Timestamp = pd.to_datetime(df.Datetime,format='%d-%m-%Y %H:%M') \n",
    "df.index = df.Timestamp \n",
    "df = df.resample('D').mean()\n",
    "train.Timestamp = pd.to_datetime(train.Datetime,format='%d-%m-%Y %H:%M') \n",
    "train.index = train.Timestamp \n",
    "train = train.resample('D').mean() \n",
    "test.Timestamp = pd.to_datetime(test.Datetime,format='%d-%m-%Y %H:%M') \n",
    "test.index = test.Timestamp \n",
    "test = test.resample('D').mean()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting data\n",
    "train.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)\n",
    "test.Count.plot(figsize=(15,8), title= 'Daily Ridership', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times we are provided with a dataset, which is stable throughout it’s time period. If we want to forecast the price for the next day, we can simply take the last day value and estimate the same value for the next day. Such forecasting technique which assumes that the next expected point is equal to the last observed point is called Naive Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd= np.asarray(train.Count)\n",
    "y_hat = test.copy()\n",
    "y_hat['naive'] = dd[len(dd)-1]\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train.index, train['Count'], label='Train')\n",
    "plt.plot(test.index,test['Count'], label='Test')\n",
    "plt.plot(y_hat.index,y_hat['naive'], label='Naive Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Naive Forecast\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(test.Count, y_hat.naive))\n",
    "print(rms)\n",
    "\n",
    "RMSE = 43.9164061439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times we are provided with a dataset, which though varies by a small margin throughout it’s time period, but the average at each time period remains constant. In such a case we can forecast the price of the next day somewhere similar to the average of all the past days.\n",
    "\n",
    "Such forecasting technique which forecasts the expected value equal to the average of all previously observed points is called Simple Average technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = test.copy()\n",
    "y_hat_avg['avg_forecast'] = train['Count'].mean()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['avg_forecast'], label='Average Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.avg_forecast))\n",
    "print(rms)\n",
    "\n",
    "RMSE = 109.545990803"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model didn’t improve our score. Hence we can infer from the score that this method works best when the average at each time period remains constant. Though the score of Naive method is better than Average method, but this does not mean that the Naive method is better than Average method on all datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many a times we are provided with a dataset, in which the prices/sales of the object increased/decreased sharply some time periods ago. In order to use the  previous Average method, we have to use the mean of all the previous data, but using all the previous data doesn’t sound right.\n",
    "\n",
    "Using the prices of the initial period would highly affect the forecast for the next period. Therefore as an improvement over simple average, we will take the average of the prices for last few time periods only. Obviously the thinking here is that only the recent values matter. Such forecasting technique which uses window of time period for calculating the average is called Moving Average technique. Calculation of the moving average involves what is sometimes called a “sliding window” of size n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = test.copy()\n",
    "y_hat_avg['moving_avg_forecast'] = train['Count'].rolling(60).mean().iloc[-1]\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['moving_avg_forecast'], label='Moving Average Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.moving_avg_forecast))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have understood the above methods, we can note that both Simple average and Weighted moving average lie on completely opposite ends. We would need something between these two extremes approaches which takes into account all the data while weighing the data points differently. For example it may be sensible to attach larger weights to more recent observations than to observations from the distant past. The technique which works on this principle is called Simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "y_hat_avg = test.copy()\n",
    "fit2 = SimpleExpSmoothing(np.asarray(train['Count'])).fit(smoothing_level=0.6,optimized=False)\n",
    "y_hat_avg['SES'] = fit2.forecast(len(test))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['SES'], label='SES')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.SES))\n",
    "print(rms)\n",
    "\n",
    "RMSE = 43.3576252252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt LInear Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use any of the above methods, it won’t take into account this trend. Trend is the general pattern of prices that we observe over a period of time. In this case we can see that there is an increasing trend.\n",
    "\n",
    "Although each one of these methods can be applied to the trend as well.  E.g. the Naive method would assume that trend between last two points is going to stay the same, or we could average all slopes between all points to get an average trend, use a moving trend average or apply exponential smoothing.\n",
    "\n",
    "But we need a method that can map the trend accurately without any assumptions. Such a method that takes into account the trend of the dataset is called Holt’s Linear Trend method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.tsa.seasonal_decompose(train.Count).plot()\n",
    "result = sm.tsa.stattools.adfuller(train.Count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Time series dataset can be decomposed into it’s componenets which are Trend, Seasonality and Residual. Any dataset that follows a trend can use Holt’s linear trend method for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = test.copy()\n",
    "\n",
    "fit1 = Holt(np.asarray(train['Count'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\n",
    "y_hat_avg['Holt_linear'] = fit1.forecast(len(test))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.Holt_linear))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt - Winters Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Datasets which show a similar set of pattern after fixed intervals of a time period suffer from seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above mentioned models don’t take into account the seasonality of the dataset while forecasting. Hence we need a method that takes into account both trend and seasonality to forecast future prices. One such algorithm that we can use in such a scenario is Holt’s Winter method. The idea behind triple exponential smoothing(Holt’s Winter) is to apply exponential smoothing to the seasonal components in addition to level and trend.\n",
    "\n",
    "Using Holt’s winter method will be the best option among the rest of the models beacuse of the seasonality factor. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations — one for the level ℓt, one for trend bt and one for the seasonal component denoted by st, with smoothing parameters α, β and γ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = test.copy()\n",
    "fit1 = ExponentialSmoothing(np.asarray(train['Count']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\n",
    "y_hat_avg['Holt_Winter'] = fit1.forecast(len(test))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot( train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.Holt_Winter))\n",
    "print(rms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common Time series model that is very popular among the Data scientists is ARIMA. It stand for Autoregressive Integrated Moving average. While exponential smoothing models were based on a description of trend and seasonality in the data, ARIMA models aim to describe the correlations in the data with each other. An improvement over ARIMA is Seasonal ARIMA. It takes into account the seasonality of dataset just like Holt’ Winter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_avg = test.copy()\n",
    "fit1 = sm.tsa.statespace.SARIMAX(train.Count, order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit()\n",
    "y_hat_avg['SARIMA'] = fit1.predict(start=\"2013-11-1\", end=\"2013-12-31\", dynamic=True)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot( train['Count'], label='Train')\n",
    "plt.plot(test['Count'], label='Test')\n",
    "plt.plot(y_hat_avg['SARIMA'], label='SARIMA')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = sqrt(mean_squared_error(test.Count, y_hat_avg.SARIMA))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using Seasonal ARIMA generates a similar solution as of Holt’s Winter. We chose the parameters as per the ACF and PACF graphs. You can learn more about them from the links provided above. If you face any difficulty finding the parameters of ARIMA model, you can use auto.arima implemented in R language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arima_project.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "result = adfuller(df.value.dropna())\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(1.00) is greater than the significance level(0.05), let’s difference the series and see how the autocorrelation plot looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n",
    "\n",
    "\n",
    "# Original Series\n",
    "fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')\n",
    "plot_acf(df.value, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(df.value.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(df.value.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(df.value.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF plot of 1st differenced series\n",
    "plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].plot(df.value.diff()); axes[0].set_title('1st Differencing')\n",
    "axes[1].set(ylim=(0,5))\n",
    "plot_pacf(df.value.diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the PACF lag 1 is quite significant since it is well above the significance line. So, we will fix the value of p as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].plot(df.value.diff()); axes[0].set_title('1st Differencing')\n",
    "axes[1].set(ylim=(0,1.2))\n",
    "plot_acf(df.value.diff().dropna(), ax=axes[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that couple of lags are well above the significance line. So, we will fix q as 2. If there is any doubt, we will go with the simpler model that sufficiently explains the Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# 1,1,2 ARIMA Model\n",
    "model = ARIMA(df.value, order=(1,1,2))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of the MA2 term is close to zero and the P-Value in ‘P>|z|’ column is highly insignificant. It should ideally be less than 0.05 for the respective X to be significant.\n",
    "\n",
    "So, we will rebuild the model without the MA2 term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,1,1 ARIMA Model\n",
    "model = ARIMA(df.value, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model AIC has slightly reduced, which is good. The p-values of the AR1 and MA1 terms have improved and are highly significant (<< 0.05).\n",
    "\n",
    "Let’s plot the residuals to ensure there are no patterns (that is, look for constant mean and variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residual errors\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Fitted\n",
    "model_fit.plot_predict(dynamic=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Out-of-Time cross-validation, we move backwards in time and forecast into the future to as many steps we took back. Then we compare the forecast against the actuals.\n",
    "\n",
    "To do so, we will create the training and testing dataset by splitting the time series into 2 contiguous parts in a reasonable proportion based on time frequency of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# Create Training and Test\n",
    "train = df.value[:85]\n",
    "test = df.value[85:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "# model = ARIMA(train, order=(3,2,1))  \n",
    "model = ARIMA(train, order=(1, 1, 1))  \n",
    "fitted = model.fit()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "fe, se, *conf = fitted.forecast(119, alpha=0.05)  # 95% conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index=test.index)\n",
    "lower_series = pd.Series(conf[:, 0], index=test.index)\n",
    "upper_series = pd.Series(conf[:, 1], index=test.index)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train, label='training')\n",
    "plt.plot(test, label='actual')\n",
    "plt.plot(fc_series, label='forecast')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.15)\n",
    "plt.title('Forecast vs Actuals')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
